{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "457e0b04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b6c3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers, regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42d5fb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path to cwd and set project root\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "\n",
    "# define full path to dataset and load\n",
    "data_path =  os.path.join(project_root, 'data/gedi_waveforms_tf.npz')\n",
    "data = np.load(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f70c389c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10546, 500, 1)\n",
      "[[[-0.92182818]\n",
      "  [-1.11135732]\n",
      "  [-1.0882749 ]\n",
      "  ...\n",
      "  [-0.82720233]\n",
      "  [-0.7545843 ]\n",
      "  [-0.65852474]]\n",
      "\n",
      " [[-0.51685445]\n",
      "  [-0.91077666]\n",
      "  [-1.0163088 ]\n",
      "  ...\n",
      "  [-1.14346151]\n",
      "  [-0.74755905]\n",
      "  [-0.30184295]]\n",
      "\n",
      " [[-0.47643436]\n",
      "  [-0.54564899]\n",
      "  [-0.33103624]\n",
      "  ...\n",
      "  [ 0.23716828]\n",
      "  [ 0.30925776]\n",
      "  [ 0.10473613]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.01867713]\n",
      "  [-1.48746914]\n",
      "  [-1.54916126]\n",
      "  ...\n",
      "  [-0.18743011]\n",
      "  [-0.19228905]\n",
      "  [-0.06010556]]\n",
      "\n",
      " [[ 0.06117187]\n",
      "  [ 0.00706989]\n",
      "  [-0.20781391]\n",
      "  ...\n",
      "  [-0.20177718]\n",
      "  [-0.17382068]\n",
      "  [-0.17141481]]\n",
      "\n",
      " [[-0.78835739]\n",
      "  [-0.72094595]\n",
      "  [-0.28164888]\n",
      "  ...\n",
      "  [ 0.97619698]\n",
      "  [ 0.64958933]\n",
      "  [ 0.17167537]]]\n"
     ]
    }
   ],
   "source": [
    "# Extract waveform data\n",
    "waveforms = data['waveforms']\n",
    "\n",
    "# Add new axis to waveform data\n",
    "waveforms = waveforms[..., np.newaxis]\n",
    "\n",
    "# inspect waveform data and shape\n",
    "print(waveforms.shape)\n",
    "print(waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "270570fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:  (7382, 500, 1)\n",
      "Testing data:  (1582, 500, 1)\n",
      "Validation data: (1582, 500, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and validation sets (80/20 split)\n",
    "x_train, x_temp = train_test_split(waveforms, test_size = 0.3, random_state = 0)\n",
    "x_test, x_val = train_test_split(x_temp, test_size = 0.5, random_state = 0)\n",
    "\n",
    "# inspect the shape of the training and validation sets\n",
    "print(f\"Training data:  {x_train.shape}\")\n",
    "print(f\"Testing data:  {x_test.shape}\")\n",
    "print(f\"Validation data: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97bc27c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, optimizers\n",
    "import numpy as np\n",
    "\n",
    "def build_autoencoder(input_shape=(500, 1), latent_shape=(4, 4),\n",
    "                                        dropout_rate=0.0, use_batchnorm=False, use_mlp_bottleneck=False):\n",
    "    \"\"\"\n",
    "    Build a convolutional autoencoder with structured latent space (latent_len, latent_dim),\n",
    "    using your proven dense bottleneck strategy.\n",
    "    \"\"\"\n",
    "\n",
    "    latent_len, latent_dim = latent_shape\n",
    "    latent_size = latent_len * latent_dim\n",
    "\n",
    "    # Encoder\n",
    "    inputs = layers.Input(shape=input_shape, name='input_layer')\n",
    "\n",
    "    x = layers.Conv1D(32, 3, padding='same')(inputs)\n",
    "    if use_batchnorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "    x = layers.Conv1D(64, 3, padding='same')(x)\n",
    "    if use_batchnorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(2, padding='same')(x)  # Output: (125, 64)\n",
    "\n",
    "    x = layers.Flatten()(x)  # shape: (125 * 64 = 8000)\n",
    "\n",
    "    if dropout_rate > 0:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Bottleneck\n",
    "    if use_mlp_bottleneck:\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "    bottleneck = layers.Dense(latent_size, activation='linear', name='bottleneck')(x)\n",
    "    reshaped_bottleneck = layers.Reshape((latent_len, latent_dim), name='latent_reshape')(bottleneck)\n",
    "\n",
    "    encoder = models.Model(inputs, reshaped_bottleneck, name='encoder')\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = layers.Input(shape=(latent_len, latent_dim), name='decoder_input')\n",
    "    x = layers.Flatten()(decoder_input)  # shape: (latent_len * latent_dim,)\n",
    "    x = layers.Dense(125 * 64, activation='relu')(x)\n",
    "    x = layers.Reshape((125, 64))(x)\n",
    "\n",
    "    x = layers.Conv1D(64, 3, padding='same')(x)\n",
    "    if use_batchnorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.UpSampling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(32, 3, padding='same')(x)\n",
    "    if use_batchnorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.UpSampling1D(2)(x)\n",
    "\n",
    "    decoded = layers.Conv1D(1, 3, padding='same', activation='linear')(x)\n",
    "\n",
    "    decoder = models.Model(decoder_input, decoded, name='decoder')\n",
    "\n",
    "    # Autoencoder\n",
    "    autoencoder_output = decoder(encoder(inputs))\n",
    "    autoencoder = models.Model(inputs, autoencoder_output, name='autoencoder')\n",
    "    autoencoder.compile(optimizer=optimizers.Adam(1e-3), loss='mse')\n",
    "\n",
    "    return autoencoder, encoder, decoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46561cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstruction_plot(model, data, experiment_id, test_loss, config, n=10, save_dir=None, seed=42):\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(project_root, 'plots')\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    index_path = os.path.join(save_dir, \"selected_indices.npy\")\n",
    "\n",
    "    # Load or generate consistent indices\n",
    "    if os.path.exists(index_path):\n",
    "        indices = np.load(index_path)\n",
    "    else:\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.choice(len(data), size=n, replace=False)\n",
    "        np.save(index_path, indices)\n",
    "\n",
    "    selected_data = data[indices]\n",
    "    reconstructions = model.predict(selected_data, verbose=0)\n",
    "\n",
    "    plt.figure(figsize=(12, 3 * n))\n",
    "\n",
    "    config_str = (\n",
    "        f\"Latent Shape: {config['latent_shape']} | Test MSE: {test_loss:.4f}\"\n",
    "    )\n",
    "    plt.suptitle(f\"{experiment_id} â€” {config_str}\", fontsize=12, y=1.02)\n",
    "\n",
    "    for i in range(n):\n",
    "        plt.subplot(n, 2, 2*i + 1)\n",
    "        plt.plot(selected_data[i].squeeze(), color='blue')\n",
    "        plt.title(f\"Original #{indices[i]}\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(n, 2, 2*i + 2)\n",
    "        plt.plot(reconstructions[i].squeeze(), color='orange')\n",
    "        plt.title(f\"Reconstructed #{indices[i]}\")\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f\"{experiment_id}_reconstruction.png\")\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f\"Saved reconstruction plot to: {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c39e3b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_configs = [\n",
    "    {\"latent_shape\": (4, 16)},\n",
    "    {\"latent_shape\": (2, 4)},\n",
    "    {\"latent_shape\": (4, 4)},\n",
    "    {\"latent_shape\": (8, 8)},\n",
    "    {\"latent_shape\": (4, 8)},\n",
    "    {\"latent_shape\": (2, 8)},\n",
    "    {\"latent_shape\": (1, 8)}  # baseline\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4451245d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# Experiment tracking\n",
    "results_log = []\n",
    "\n",
    "def run_latent_experiment(\n",
    "    experiment_id,\n",
    "    latent_shape,\n",
    "    input_shape=(500, 1),\n",
    "    learning_rate=1e-3,\n",
    "    dropout_rate=0.0,\n",
    "    batch_size=64,\n",
    "    epochs=20,\n",
    "    save_models=True,\n",
    "    results_path=os.path.join(project_root, \"models/latent_shape_experiment_results.csv\")\n",
    "):\n",
    "    print(f\"\\nRunning {experiment_id} | Latent Shape: {latent_shape}, LR: {learning_rate}, Dropout: {dropout_rate}\")\n",
    "\n",
    "    # Build model\n",
    "    autoencoder, encoder, decoder = build_autoencoder(\n",
    "        input_shape=input_shape,\n",
    "        latent_shape=latent_shape,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_batchnorm = False,\n",
    "        use_mlp_bottleneck = False\n",
    "    )\n",
    "\n",
    "    autoencoder.compile(optimizer=optimizers.Adam(learning_rate), loss='mse')\n",
    "\n",
    "    # Set up TensorBoard\n",
    "    log_dir = os.path.join(project_root, f\"logs/{experiment_id}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "    tensorboard_cb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    # Train model\n",
    "    history = autoencoder.fit(\n",
    "        x_train, x_train,\n",
    "        validation_data=(x_test, x_test),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        callbacks=[tensorboard_cb]\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    test_loss = autoencoder.evaluate(x_test, x_test, verbose=0)\n",
    "\n",
    "    # Save model + embeddings (optional)\n",
    "    if save_models:\n",
    "        model_dir = os.path.join(project_root, \"models\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        autoencoder.save(os.path.join(model_dir, f\"autoencoder_{experiment_id}.keras\"))\n",
    "        encoder.save(os.path.join(model_dir, f\"encoder_{experiment_id}.keras\"))\n",
    "        embeddings = encoder.predict(waveforms, verbose=0)\n",
    "        np.save(os.path.join(model_dir, f\"embeddings_{experiment_id}.npy\"), embeddings)\n",
    "\n",
    "    # Save reconstruction plot\n",
    "    save_reconstruction_plot(\n",
    "        model=autoencoder,\n",
    "        data=x_test,\n",
    "        experiment_id=experiment_id,\n",
    "        test_loss=test_loss,\n",
    "        config={'latent_shape': latent_shape},\n",
    "        n=10,\n",
    "        save_dir=os.path.join(project_root, \"plots\")\n",
    "    )\n",
    "\n",
    "    # Log results\n",
    "    result = {\n",
    "        'Experiment': experiment_id,\n",
    "        'Latent Shape': str(latent_shape),\n",
    "        'LR': learning_rate,\n",
    "        'Dropout': dropout_rate,\n",
    "        'Epochs': epochs,\n",
    "        'Train Loss': train_loss,\n",
    "        'Val Loss': val_loss,\n",
    "        'Test Loss': test_loss\n",
    "    }\n",
    "    results_log.append(result)\n",
    "\n",
    "    df_results = pd.DataFrame(results_log)\n",
    "    df_results.to_csv(results_path, index=False)\n",
    "    print(f\"{experiment_id} complete â€” Test MSE: {test_loss:.4f} â€” Logged to {results_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98d218cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running exp_01_4x16 | Latent Shape: (4, 16), LR: 0.001, Dropout: 0.0\n",
      "Saved reconstruction plot to: c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\plots\\exp_01_4x16_reconstruction.png\n",
      "exp_01_4x16 complete â€” Test MSE: 0.0164 â€” Logged to c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\models/latent_shape_experiment_results.csv\n",
      "\n",
      "Running exp_02_2x4 | Latent Shape: (2, 4), LR: 0.001, Dropout: 0.0\n",
      "Saved reconstruction plot to: c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\plots\\exp_02_2x4_reconstruction.png\n",
      "exp_02_2x4 complete â€” Test MSE: 0.0458 â€” Logged to c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\models/latent_shape_experiment_results.csv\n",
      "\n",
      "Running exp_03_4x4 | Latent Shape: (4, 4), LR: 0.001, Dropout: 0.0\n",
      "Saved reconstruction plot to: c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\plots\\exp_03_4x4_reconstruction.png\n",
      "exp_03_4x4 complete â€” Test MSE: 0.0331 â€” Logged to c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\models/latent_shape_experiment_results.csv\n",
      "\n",
      "Running exp_04_8x8 | Latent Shape: (8, 8), LR: 0.001, Dropout: 0.0\n",
      "Saved reconstruction plot to: c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\plots\\exp_04_8x8_reconstruction.png\n",
      "exp_04_8x8 complete â€” Test MSE: 0.0171 â€” Logged to c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\models/latent_shape_experiment_results.csv\n",
      "\n",
      "Running exp_05_4x8 | Latent Shape: (4, 8), LR: 0.001, Dropout: 0.0\n",
      "Saved reconstruction plot to: c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\plots\\exp_05_4x8_reconstruction.png\n",
      "exp_05_4x8 complete â€” Test MSE: 0.0239 â€” Logged to c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\models/latent_shape_experiment_results.csv\n",
      "\n",
      "Running exp_06_2x8 | Latent Shape: (2, 8), LR: 0.001, Dropout: 0.0\n",
      "Saved reconstruction plot to: c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\plots\\exp_06_2x8_reconstruction.png\n",
      "exp_06_2x8 complete â€” Test MSE: 0.0332 â€” Logged to c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\models/latent_shape_experiment_results.csv\n",
      "\n",
      "Running exp_07_1x8 | Latent Shape: (1, 8), LR: 0.001, Dropout: 0.0\n",
      "Saved reconstruction plot to: c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\plots\\exp_07_1x8_reconstruction.png\n",
      "exp_07_1x8 complete â€” Test MSE: 0.0452 â€” Logged to c:\\Users\\Zachary\\Downloads\\gedi_waveform_processor_library\\models/latent_shape_experiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "start_idx = 1 \n",
    "for i, config in enumerate(latent_configs):\n",
    "    latent_shape = config['latent_shape']\n",
    "    experiment_id = f\"exp_{(start_idx + i):02d}_{latent_shape[0]}x{latent_shape[1]}\"\n",
    "    \n",
    "    run_latent_experiment(\n",
    "        experiment_id=experiment_id,\n",
    "        latent_shape=latent_shape,\n",
    "        input_shape=(500, 1),\n",
    "        learning_rate=1e-3,\n",
    "        dropout_rate=0.0,\n",
    "        batch_size=64,\n",
    "        epochs=30,\n",
    "        save_models=True,\n",
    "        results_path=os.path.join(project_root, \"models/latent_shape_experiment_results.csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d8eb50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_latent_experiment(\n",
    "#     experiment_id=\"latent_10x8\",\n",
    "#     latent_shape=(10, 8),\n",
    "#     input_shape=(500, 1),\n",
    "#     learning_rate=1e-3,\n",
    "#     dropout_rate=0.0,\n",
    "#     batch_size=64,\n",
    "#     epochs=30\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9facbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gedi_pro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
