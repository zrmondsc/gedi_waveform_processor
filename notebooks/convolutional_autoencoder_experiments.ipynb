{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Autoencoder Experimentation Notebook\n",
    "\n",
    "This notebook is designed for **systematic experimentation** with convolutional autoencoders trained on GEDI waveform data. \n",
    "It allows the user to quickly test different encoder architectures, embedding dimensions, and training parameters while logging results and saving models/embeddings.\n",
    "\n",
    "ðŸŽ¯ Goals:\n",
    "Build a compact autoencoder that:\n",
    "\n",
    "- Reconstructs GEDI waveforms with moderate - high fidelity\n",
    "\n",
    "- Outputs low-dimensional embeddings that can be regressed from Sentinel imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# import ML libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "\n",
    "# import visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load and Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path to cwd and set project root\n",
    "notebook_dir = Path.cwd()\n",
    "project_root = notebook_dir.parent\n",
    "\n",
    "# define full path to dataset and load\n",
    "data_path =  os.path.join(project_root, 'data/gedi_waveforms_tf.npz')\n",
    "data = np.load(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10546, 500, 1)\n",
      "[[[-0.92182818]\n",
      "  [-1.11135732]\n",
      "  [-1.0882749 ]\n",
      "  ...\n",
      "  [-0.82720233]\n",
      "  [-0.7545843 ]\n",
      "  [-0.65852474]]\n",
      "\n",
      " [[-0.51685445]\n",
      "  [-0.91077666]\n",
      "  [-1.0163088 ]\n",
      "  ...\n",
      "  [-1.14346151]\n",
      "  [-0.74755905]\n",
      "  [-0.30184295]]\n",
      "\n",
      " [[-0.47643436]\n",
      "  [-0.54564899]\n",
      "  [-0.33103624]\n",
      "  ...\n",
      "  [ 0.23716828]\n",
      "  [ 0.30925776]\n",
      "  [ 0.10473613]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.01867713]\n",
      "  [-1.48746914]\n",
      "  [-1.54916126]\n",
      "  ...\n",
      "  [-0.18743011]\n",
      "  [-0.19228905]\n",
      "  [-0.06010556]]\n",
      "\n",
      " [[ 0.06117187]\n",
      "  [ 0.00706989]\n",
      "  [-0.20781391]\n",
      "  ...\n",
      "  [-0.20177718]\n",
      "  [-0.17382068]\n",
      "  [-0.17141481]]\n",
      "\n",
      " [[-0.78835739]\n",
      "  [-0.72094595]\n",
      "  [-0.28164888]\n",
      "  ...\n",
      "  [ 0.97619698]\n",
      "  [ 0.64958933]\n",
      "  [ 0.17167537]]]\n"
     ]
    }
   ],
   "source": [
    "# Extract waveform data\n",
    "waveforms = data['waveforms']\n",
    "\n",
    "# Add new axis to waveform data\n",
    "waveforms = waveforms[..., np.newaxis]\n",
    "\n",
    "# inspect waveform data and shape\n",
    "print(waveforms.shape)\n",
    "print(waveforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data:  (8436, 500, 1)\n",
      "Testing data:  (1055, 500, 1)\n",
      "Validation data: (1055, 500, 1)\n"
     ]
    }
   ],
   "source": [
    "# Split dataset into training and validation sets (80/20 split)\n",
    "x_train, x_temp = train_test_split(waveforms, test_size = 0.2, random_state = 0)\n",
    "x_test, x_val = train_test_split(x_temp, test_size = 0.5, random_state = 0)\n",
    "\n",
    "# inspect the shape of the training and validation sets\n",
    "print(f\"Training data:  {x_train.shape}\")\n",
    "print(f\"Testing data:  {x_test.shape}\")\n",
    "print(f\"Validation data: {x_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Autoencoder Model\n",
    "This function constructs the encoder-decoder model using configurable parameters:\n",
    "- `latent_dim`: size of the final embedding layer\n",
    "- `use_global_avg`: whether to use GlobalAveragePooling or Flatten before the bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_autoencoder(input_shape, latent_dim=16, use_global_avg=False,\n",
    "                      dropout_rate=0.0, use_batchnorm=False, bottleneck_type='dense'):\n",
    "    \"\"\"\n",
    "    Build a convolutional autoencoder with flexible bottleneck and optional batchnorm/dropout.\n",
    "\n",
    "    Parameters:\n",
    "        input_shape (tuple): Shape of input waveform (e.g., (500, 1))\n",
    "        latent_dim (int): Size of bottleneck representation\n",
    "        use_global_avg (bool): Use GlobalAveragePooling1D before bottleneck if True\n",
    "        dropout_rate (float): Dropout rate before bottleneck\n",
    "        use_batchnorm (bool): If True, applies BatchNormalization after each Conv1D\n",
    "        bottleneck_type (str): 'dense' for a single Dense layer, 'mlp' for 2-layer bottleneck\n",
    "\n",
    "    Returns:\n",
    "        (autoencoder, encoder): Keras models\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    inputs = layers.Input(shape=input_shape, name='input_layer')\n",
    "    x = layers.Conv1D(32, 3, padding='same')(inputs)\n",
    "    if use_batchnorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "    x = layers.Conv1D(64, 3, padding='same')(x)\n",
    "\n",
    "    if use_batchnorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling1D(2, padding='same')(x)\n",
    "\n",
    "    # Flatten or Global Average before bottleneck\n",
    "    if use_global_avg:\n",
    "        x = layers.GlobalAveragePooling1D()(x)  # â†’ (64,)\n",
    "    else:\n",
    "        x = layers.Flatten()(x)                 # â†’ (125 * 64 = 8000,)\n",
    "\n",
    "    if dropout_rate > 0:\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "\n",
    "    # Bottleneck \n",
    "    if bottleneck_type == 'mlp':\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        bottleneck = layers.Dense(latent_dim, activation='linear', name='bottleneck')(x)\n",
    "    else:\n",
    "        bottleneck = layers.Dense(latent_dim, activation='linear', name='bottleneck')(x)\n",
    "\n",
    "    # Decoder\n",
    "    x = layers.Dense(125 * 64, activation='relu')(bottleneck)\n",
    "    x = layers.Reshape((125, 64))(x)\n",
    "    x = layers.Conv1D(64, 3, padding='same')(x)\n",
    "\n",
    "    if use_batchnorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.UpSampling1D(2)(x)\n",
    "\n",
    "    x = layers.Conv1D(32, 3, padding='same')(x)\n",
    "\n",
    "    if use_batchnorm:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.UpSampling1D(2)(x)\n",
    "\n",
    "    decoded = layers.Conv1D(1, 3, activation='linear', padding='same')(x)\n",
    "\n",
    "    autoencoder = models.Model(inputs, decoded, name='autoencoder')\n",
    "    encoder = models.Model(inputs, bottleneck, name='encoder')\n",
    "\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  4. Run and Log Experiments\n",
    "This function trains the autoencoder, saves the model and encoder, and logs performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_reconstruction_plot(model, data, experiment_id, test_loss, config, n=10, save_dir=None, seed=42):\n",
    "    \"\"\"\n",
    "    Save a side-by-side plot of original vs reconstructed waveforms with experiment metadata.\n",
    "    Consistently uses the same n random indices across experiments (saved to .npy).\n",
    "\n",
    "    Parameters:\n",
    "        model: Trained autoencoder\n",
    "        data: Input waveforms\n",
    "        experiment_id: UID\n",
    "        test_loss: test MSE to annotate on plot\n",
    "        config: Dict of model parameters\n",
    "        n: Number of waveform plots\n",
    "        save_dir: Output directory\n",
    "        seed: Random seed\n",
    "    \"\"\"\n",
    "    if save_dir is None:\n",
    "        save_dir = os.path.join(project_root, 'models/plots')\n",
    "    os.makedirs(save_dir, exist_ok = True)\n",
    "\n",
    "    index_path = os.path.join(save_dir, \"selected_indices.npy\")\n",
    "\n",
    "    # Load or generate consistent indices\n",
    "    if os.path.exists(index_path):\n",
    "        indices = np.load(index_path)\n",
    "    else:\n",
    "        np.random.seed(seed)\n",
    "        indices = np.random.choice(len(data), size=n, replace=False)\n",
    "        np.save(index_path, indices)\n",
    "\n",
    "    selected_data = data[indices]\n",
    "    reconstructions = model.predict(selected_data)\n",
    "\n",
    "    plt.figure(figsize=(12, 3 * n))\n",
    "\n",
    "    # Annotate with experiment config\n",
    "    config_str = (\n",
    "        f\"Latent Dim: {config['latent_dim']} | \"\n",
    "        f\"Dropout: {config['dropout_rate']} | \"\n",
    "        f\"BatchNorm: {config['use_batchnorm']} | \"\n",
    "        f\"Bottleneck: {config['bottleneck_type']} | \"\n",
    "        f\"Test MSE: {test_loss:.4f}\"\n",
    "    )\n",
    "    plt.suptitle(f\"{experiment_id} â€” {config_str}\", fontsize=12, y=1.02)\n",
    "\n",
    "    for i in range(n):\n",
    "        plt.subplot(n, 2, 2*i + 1)\n",
    "        plt.plot(selected_data[i].squeeze(), color='blue')\n",
    "        plt.title(f\"Original #{indices[i]}\")\n",
    "        plt.grid(True)\n",
    "\n",
    "        plt.subplot(n, 2, 2*i + 2)\n",
    "        plt.plot(reconstructions[i].squeeze(), color='orange')\n",
    "        plt.title(f\"Reconstructed #{indices[i]}\")\n",
    "        plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f\"{experiment_id}_reconstructions.png\")\n",
    "    plt.savefig(save_path, bbox_inches='tight')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_log = []\n",
    "\n",
    "def run_experiment(\n",
    "    experiment_id,\n",
    "    input_shape,\n",
    "    latent_dim=16,\n",
    "    use_global_avg=False,\n",
    "    learning_rate=1e-3,\n",
    "    dropout_rate=0.0,\n",
    "    use_batchnorm=False,\n",
    "    bottleneck_type='dense',\n",
    "    epochs=20,\n",
    "    batch_size=64,\n",
    "    results_path=os.path.join(project_root, \"models/experiment_results.csv\")\n",
    "):\n",
    "    print(f\"\\nðŸš€ Running {experiment_id} | Latent Dim: {latent_dim}, AvgPool: {use_global_avg}, \"\n",
    "          f\"LR: {learning_rate}, Dropout: {dropout_rate}, BatchNorm: {use_batchnorm}, Bottleneck: {bottleneck_type}\")\n",
    "\n",
    "    # Build model\n",
    "    autoencoder, encoder = build_autoencoder(\n",
    "        input_shape=input_shape,\n",
    "        latent_dim=latent_dim,\n",
    "        use_global_avg=use_global_avg,\n",
    "        dropout_rate=dropout_rate,\n",
    "        use_batchnorm=use_batchnorm,\n",
    "        bottleneck_type=bottleneck_type\n",
    "    )\n",
    "    autoencoder.compile(optimizer=optimizers.Adam(learning_rate), loss='mse')\n",
    "\n",
    "    # Set up TensorBoard\n",
    "    log_dir = os.path.join(project_root, f\"logs/{experiment_id}_{datetime.now().strftime('%Y%m%d-%H%M%S')}\")\n",
    "    tensorboard_cb = TensorBoard(log_dir=log_dir)\n",
    "\n",
    "    # Train model\n",
    "    history = autoencoder.fit(\n",
    "        x_train, x_train,\n",
    "        validation_data=(x_val, x_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        verbose=0,\n",
    "        callbacks=[tensorboard_cb]\n",
    "    )\n",
    "\n",
    "    # Evaluate\n",
    "    train_loss = history.history['loss'][-1]\n",
    "    val_loss = history.history['val_loss'][-1]\n",
    "    test_loss = autoencoder.evaluate(x_test, x_test, verbose=0)\n",
    "\n",
    "    # Save model + embeddings\n",
    "    autoencoder.save(os.path.join(project_root, f\"models/autoencoder_{experiment_id}.keras\"))\n",
    "    encoder.save(os.path.join(project_root, f\"models/encoder_{experiment_id}.keras\"))\n",
    "    embeddings = encoder.predict(waveforms)\n",
    "    np.save(os.path.join(project_root, f\"models/embeddings_{experiment_id}.npy\"), embeddings)\n",
    "\n",
    "    # Log results\n",
    "    result = {\n",
    "        'Experiment': experiment_id,\n",
    "        'Latent Dim': latent_dim,\n",
    "        'Global Avg?': use_global_avg,\n",
    "        'LR': learning_rate,\n",
    "        'Dropout': dropout_rate,\n",
    "        'BatchNorm': use_batchnorm,\n",
    "        'Bottleneck': bottleneck_type,\n",
    "        'Epochs': epochs,\n",
    "        'Train Loss': train_loss,\n",
    "        'Val Loss': val_loss,\n",
    "        'Test Loss': test_loss\n",
    "    }\n",
    "    results_log.append(result)\n",
    "\n",
    "    # Save to CSV\n",
    "    df_results = pd.DataFrame(results_log)\n",
    "    df_results.to_csv(results_path, index=False)\n",
    "\n",
    "    # Save a reconstruction plot (10 samples with consistent indices)\n",
    "    save_reconstruction_plot(\n",
    "        model=autoencoder,\n",
    "        data=x_val,\n",
    "        experiment_id=experiment_id,\n",
    "        test_loss=test_loss,\n",
    "        config={\n",
    "            'latent_dim': latent_dim,\n",
    "            'dropout_rate': dropout_rate,\n",
    "            'use_batchnorm': use_batchnorm,\n",
    "            'bottleneck_type': bottleneck_type\n",
    "        },\n",
    "        n=10,\n",
    "        save_dir=os.path.join(project_root, \"plots\")\n",
    "    )\n",
    "\n",
    "    print(f\"{experiment_id} complete â€” Test MSE: {test_loss:.4f} â€” Logged to {results_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define and Run a Set of Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'experiment_id': 'exp_01',\n",
       "  'latent_dim': 4,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0,\n",
       "  'use_batchnorm': False,\n",
       "  'bottleneck_type': 'dense'},\n",
       " {'experiment_id': 'exp_02',\n",
       "  'latent_dim': 4,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0,\n",
       "  'use_batchnorm': False,\n",
       "  'bottleneck_type': 'mlp'},\n",
       " {'experiment_id': 'exp_03',\n",
       "  'latent_dim': 4,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0,\n",
       "  'use_batchnorm': True,\n",
       "  'bottleneck_type': 'dense'},\n",
       " {'experiment_id': 'exp_04',\n",
       "  'latent_dim': 4,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0,\n",
       "  'use_batchnorm': True,\n",
       "  'bottleneck_type': 'mlp'},\n",
       " {'experiment_id': 'exp_05',\n",
       "  'latent_dim': 4,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.2,\n",
       "  'use_batchnorm': False,\n",
       "  'bottleneck_type': 'dense'},\n",
       " {'experiment_id': 'exp_06',\n",
       "  'latent_dim': 4,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.2,\n",
       "  'use_batchnorm': False,\n",
       "  'bottleneck_type': 'mlp'},\n",
       " {'experiment_id': 'exp_07',\n",
       "  'latent_dim': 4,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.2,\n",
       "  'use_batchnorm': True,\n",
       "  'bottleneck_type': 'dense'},\n",
       " {'experiment_id': 'exp_08',\n",
       "  'latent_dim': 4,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.2,\n",
       "  'use_batchnorm': True,\n",
       "  'bottleneck_type': 'mlp'},\n",
       " {'experiment_id': 'exp_09',\n",
       "  'latent_dim': 8,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0,\n",
       "  'use_batchnorm': False,\n",
       "  'bottleneck_type': 'dense'},\n",
       " {'experiment_id': 'exp_10',\n",
       "  'latent_dim': 8,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0,\n",
       "  'use_batchnorm': False,\n",
       "  'bottleneck_type': 'mlp'},\n",
       " {'experiment_id': 'exp_11',\n",
       "  'latent_dim': 8,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0,\n",
       "  'use_batchnorm': True,\n",
       "  'bottleneck_type': 'dense'},\n",
       " {'experiment_id': 'exp_12',\n",
       "  'latent_dim': 8,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0,\n",
       "  'use_batchnorm': True,\n",
       "  'bottleneck_type': 'mlp'},\n",
       " {'experiment_id': 'exp_13',\n",
       "  'latent_dim': 8,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.2,\n",
       "  'use_batchnorm': False,\n",
       "  'bottleneck_type': 'dense'},\n",
       " {'experiment_id': 'exp_14',\n",
       "  'latent_dim': 8,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.2,\n",
       "  'use_batchnorm': False,\n",
       "  'bottleneck_type': 'mlp'},\n",
       " {'experiment_id': 'exp_15',\n",
       "  'latent_dim': 8,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.2,\n",
       "  'use_batchnorm': True,\n",
       "  'bottleneck_type': 'dense'},\n",
       " {'experiment_id': 'exp_16',\n",
       "  'latent_dim': 8,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.2,\n",
       "  'use_batchnorm': True,\n",
       "  'bottleneck_type': 'mlp'},\n",
       " {'experiment_id': 'exp_17',\n",
       "  'latent_dim': 16,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0,\n",
       "  'use_batchnorm': False,\n",
       "  'bottleneck_type': 'dense'},\n",
       " {'experiment_id': 'exp_18',\n",
       "  'latent_dim': 16,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0,\n",
       "  'use_batchnorm': False,\n",
       "  'bottleneck_type': 'mlp'},\n",
       " {'experiment_id': 'exp_19',\n",
       "  'latent_dim': 16,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0,\n",
       "  'use_batchnorm': True,\n",
       "  'bottleneck_type': 'dense'},\n",
       " {'experiment_id': 'exp_20',\n",
       "  'latent_dim': 16,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.0,\n",
       "  'use_batchnorm': True,\n",
       "  'bottleneck_type': 'mlp'},\n",
       " {'experiment_id': 'exp_21',\n",
       "  'latent_dim': 16,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.2,\n",
       "  'use_batchnorm': False,\n",
       "  'bottleneck_type': 'dense'},\n",
       " {'experiment_id': 'exp_22',\n",
       "  'latent_dim': 16,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.2,\n",
       "  'use_batchnorm': False,\n",
       "  'bottleneck_type': 'mlp'},\n",
       " {'experiment_id': 'exp_23',\n",
       "  'latent_dim': 16,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.2,\n",
       "  'use_batchnorm': True,\n",
       "  'bottleneck_type': 'dense'},\n",
       " {'experiment_id': 'exp_24',\n",
       "  'latent_dim': 16,\n",
       "  'use_global_avg': False,\n",
       "  'learning_rate': 0.001,\n",
       "  'dropout_rate': 0.2,\n",
       "  'use_batchnorm': True,\n",
       "  'bottleneck_type': 'mlp'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "experiments = []\n",
    "\n",
    "latent_dims = [4, 8, 16]\n",
    "dropouts = [0.0, 0.2]\n",
    "batchnorms = [False, True]\n",
    "bottlenecks = ['dense', 'mlp']\n",
    "\n",
    "exp_id = 1\n",
    "\n",
    "for ld in latent_dims:\n",
    "    for dr in dropouts:\n",
    "        for bn in batchnorms:\n",
    "            for bt in bottlenecks:\n",
    "                experiments.append({\n",
    "                    'experiment_id': f\"exp_{exp_id:02d}\",\n",
    "                    'latent_dim': ld,\n",
    "                    'use_global_avg': False,         # Keep this off for now (better performance)\n",
    "                    'learning_rate': 1e-3,\n",
    "                    'dropout_rate': dr,\n",
    "                    'use_batchnorm': bn,\n",
    "                    'bottleneck_type': bt\n",
    "                })\n",
    "                exp_id += 1\n",
    "experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in experiments:\n",
    "    run_experiment(\n",
    "        experiment_id=exp['experiment_id'],\n",
    "        input_shape=(500, 1),\n",
    "        latent_dim=exp['latent_dim'],\n",
    "        use_global_avg=exp['use_global_avg'],\n",
    "        learning_rate=exp['learning_rate'],\n",
    "        dropout_rate=exp['dropout_rate'],\n",
    "        use_batchnorm=exp['use_batchnorm'],\n",
    "        bottleneck_type=exp['bottleneck_type'],\n",
    "        epochs=10\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Experiment</th>\n",
       "      <th>Latent Dim</th>\n",
       "      <th>Global Avg?</th>\n",
       "      <th>LR</th>\n",
       "      <th>Dropout</th>\n",
       "      <th>BatchNorm</th>\n",
       "      <th>Bottleneck</th>\n",
       "      <th>Epochs</th>\n",
       "      <th>Train Loss</th>\n",
       "      <th>Val Loss</th>\n",
       "      <th>Test Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>exp_01</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>dense</td>\n",
       "      <td>10</td>\n",
       "      <td>0.069322</td>\n",
       "      <td>0.066939</td>\n",
       "      <td>0.071382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exp_02</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>mlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.067434</td>\n",
       "      <td>0.065875</td>\n",
       "      <td>0.070250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>exp_03</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>dense</td>\n",
       "      <td>10</td>\n",
       "      <td>0.064839</td>\n",
       "      <td>0.062457</td>\n",
       "      <td>0.067398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>exp_04</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>mlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.062686</td>\n",
       "      <td>0.064171</td>\n",
       "      <td>0.068558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>exp_05</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>dense</td>\n",
       "      <td>10</td>\n",
       "      <td>0.069728</td>\n",
       "      <td>0.066244</td>\n",
       "      <td>0.070839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>exp_06</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>mlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.069151</td>\n",
       "      <td>0.067018</td>\n",
       "      <td>0.070095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>exp_07</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>dense</td>\n",
       "      <td>10</td>\n",
       "      <td>0.065186</td>\n",
       "      <td>0.063029</td>\n",
       "      <td>0.067322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>exp_08</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>mlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.068641</td>\n",
       "      <td>0.086180</td>\n",
       "      <td>0.090518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>exp_09</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>dense</td>\n",
       "      <td>10</td>\n",
       "      <td>0.044398</td>\n",
       "      <td>0.044744</td>\n",
       "      <td>0.047653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>exp_10</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>mlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.044359</td>\n",
       "      <td>0.044952</td>\n",
       "      <td>0.047301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>exp_11</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>dense</td>\n",
       "      <td>10</td>\n",
       "      <td>0.045972</td>\n",
       "      <td>0.048071</td>\n",
       "      <td>0.051182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>exp_12</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>mlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.044777</td>\n",
       "      <td>0.045979</td>\n",
       "      <td>0.047998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>exp_13</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>dense</td>\n",
       "      <td>10</td>\n",
       "      <td>0.045971</td>\n",
       "      <td>0.045663</td>\n",
       "      <td>0.047759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>exp_14</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>mlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.045206</td>\n",
       "      <td>0.044980</td>\n",
       "      <td>0.047356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>exp_15</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>dense</td>\n",
       "      <td>10</td>\n",
       "      <td>0.046168</td>\n",
       "      <td>0.047524</td>\n",
       "      <td>0.050173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>exp_16</td>\n",
       "      <td>8</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>mlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.046781</td>\n",
       "      <td>0.045955</td>\n",
       "      <td>0.048959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>exp_17</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>dense</td>\n",
       "      <td>10</td>\n",
       "      <td>0.030683</td>\n",
       "      <td>0.032531</td>\n",
       "      <td>0.034041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>exp_18</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>mlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.031270</td>\n",
       "      <td>0.032527</td>\n",
       "      <td>0.034903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>exp_19</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>dense</td>\n",
       "      <td>10</td>\n",
       "      <td>0.033183</td>\n",
       "      <td>0.035332</td>\n",
       "      <td>0.037167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>exp_20</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "      <td>True</td>\n",
       "      <td>mlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.032459</td>\n",
       "      <td>0.036007</td>\n",
       "      <td>0.038186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>exp_21</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>dense</td>\n",
       "      <td>10</td>\n",
       "      <td>0.032973</td>\n",
       "      <td>0.032244</td>\n",
       "      <td>0.034302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>exp_22</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>False</td>\n",
       "      <td>mlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.034105</td>\n",
       "      <td>0.033688</td>\n",
       "      <td>0.036138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>exp_23</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>dense</td>\n",
       "      <td>10</td>\n",
       "      <td>0.034499</td>\n",
       "      <td>0.034225</td>\n",
       "      <td>0.036638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>exp_24</td>\n",
       "      <td>16</td>\n",
       "      <td>False</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.2</td>\n",
       "      <td>True</td>\n",
       "      <td>mlp</td>\n",
       "      <td>10</td>\n",
       "      <td>0.034520</td>\n",
       "      <td>0.034883</td>\n",
       "      <td>0.037205</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Experiment  Latent Dim  Global Avg?     LR  Dropout  BatchNorm Bottleneck  \\\n",
       "0      exp_01           4        False  0.001      0.0      False      dense   \n",
       "1      exp_02           4        False  0.001      0.0      False        mlp   \n",
       "2      exp_03           4        False  0.001      0.0       True      dense   \n",
       "3      exp_04           4        False  0.001      0.0       True        mlp   \n",
       "4      exp_05           4        False  0.001      0.2      False      dense   \n",
       "5      exp_06           4        False  0.001      0.2      False        mlp   \n",
       "6      exp_07           4        False  0.001      0.2       True      dense   \n",
       "7      exp_08           4        False  0.001      0.2       True        mlp   \n",
       "8      exp_09           8        False  0.001      0.0      False      dense   \n",
       "9      exp_10           8        False  0.001      0.0      False        mlp   \n",
       "10     exp_11           8        False  0.001      0.0       True      dense   \n",
       "11     exp_12           8        False  0.001      0.0       True        mlp   \n",
       "12     exp_13           8        False  0.001      0.2      False      dense   \n",
       "13     exp_14           8        False  0.001      0.2      False        mlp   \n",
       "14     exp_15           8        False  0.001      0.2       True      dense   \n",
       "15     exp_16           8        False  0.001      0.2       True        mlp   \n",
       "16     exp_17          16        False  0.001      0.0      False      dense   \n",
       "17     exp_18          16        False  0.001      0.0      False        mlp   \n",
       "18     exp_19          16        False  0.001      0.0       True      dense   \n",
       "19     exp_20          16        False  0.001      0.0       True        mlp   \n",
       "20     exp_21          16        False  0.001      0.2      False      dense   \n",
       "21     exp_22          16        False  0.001      0.2      False        mlp   \n",
       "22     exp_23          16        False  0.001      0.2       True      dense   \n",
       "23     exp_24          16        False  0.001      0.2       True        mlp   \n",
       "\n",
       "    Epochs  Train Loss  Val Loss  Test Loss  \n",
       "0       10    0.069322  0.066939   0.071382  \n",
       "1       10    0.067434  0.065875   0.070250  \n",
       "2       10    0.064839  0.062457   0.067398  \n",
       "3       10    0.062686  0.064171   0.068558  \n",
       "4       10    0.069728  0.066244   0.070839  \n",
       "5       10    0.069151  0.067018   0.070095  \n",
       "6       10    0.065186  0.063029   0.067322  \n",
       "7       10    0.068641  0.086180   0.090518  \n",
       "8       10    0.044398  0.044744   0.047653  \n",
       "9       10    0.044359  0.044952   0.047301  \n",
       "10      10    0.045972  0.048071   0.051182  \n",
       "11      10    0.044777  0.045979   0.047998  \n",
       "12      10    0.045971  0.045663   0.047759  \n",
       "13      10    0.045206  0.044980   0.047356  \n",
       "14      10    0.046168  0.047524   0.050173  \n",
       "15      10    0.046781  0.045955   0.048959  \n",
       "16      10    0.030683  0.032531   0.034041  \n",
       "17      10    0.031270  0.032527   0.034903  \n",
       "18      10    0.033183  0.035332   0.037167  \n",
       "19      10    0.032459  0.036007   0.038186  \n",
       "20      10    0.032973  0.032244   0.034302  \n",
       "21      10    0.034105  0.033688   0.036138  \n",
       "22      10    0.034499  0.034225   0.036638  \n",
       "23      10    0.034520  0.034883   0.037205  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_results = pd.read_csv(os.path.join(project_root, \"models/experiment_results.csv\"))\n",
    "exp_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gedi_pro_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
